---
title: "homework5_isye6501"
author: "Zach Olivier"
date: "6/9/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# set up directory and packages
setwd('C:/Users/zolivier/Desktop/Homework 5')

# load packages
pacman::p_load(tidyverse, kernlab, caret, kknn, modelr, ggthemes, corrplot, MASS, DMwR, FrF2)

```

## Question 11.1

Question:

Using the crime data set uscrime.txt from Questions 8.2, 9.1, and 10.1, build a regression model using:
1. Stepwise regression 2. Lasso 3. Elastic net For Parts 2 and 3, remember to scale the data first - otherwise, the regression coefficients will be on different scales and the constraint won't have the desired effect. 
For Parts 2 and 3, use the glmnet function in R

<br>

Answer:

Below are my steps to fit a stepwise regression, lasso regression, and elastic net regression to the crime data set.

We start by reading in the data and then transform the data using caret's PreProcess function. Then I develop training and test data sets to prepare for fitting each model. We can use the same training data to fit each model and judge their relative performance via cross validation (k = 10).

Output from each model and the best tuned model's coefficients are shown below. 

Here are the results:

    - Stepwise: R^2 = 82%
    - LASSO: R^2 = 81% (alpha = 1, lambda = .8)
    - Elastic Net: R^2 = 80% (aplha = .5, lambda = 5.5)
    
**Overall it seems the stepwise regression performs the best on our training dataset. It will be interesting to see if these results generalize to new data, the regularization of the LASSO and Elastic Net may boost test set accuracy. It is also interesting that in both the LASSO and Elastic Net model regularization is fairly small. Time is the only variable that is taken completely to zero by LASSO and Elastic Net with such a small regularization penalty.**
    



<br>

``` {r glmnet, message = FALSE}

set.seed(110)

# read in the crime data
crime_df = read_delim('11.1uscrimeSummer2018.txt', delim = '\t') %>% 
        as.data.frame()


# using cool caret functions to transform predictor data 
transform_df = caret::preProcess(
        crime_df %>% dplyr::select(., -Crime), 
        method = c('center', 'scale', 'nzv')
        )

crime_mod_df <- predict(transform_df, crime_df)


# set up train and testing split
train <- createDataPartition(crime_mod_df$Crime, p = .75, list = F)

# set up test and train datasets
crime_train <- crime_mod_df[train,]
crime_test <- crime_mod_df[-train,]

# check splits
dim(crime_train); dim(crime_test)


set.seed(110)


# fit model - stepwise
crime_step <- train(
        Crime ~ ., 
        data = crime_train,
        method = 'glmStepAIC',
        trControl = trainControl(method = 'cv'),
        trace = 0
        )

coef(crime_step$finalModel)

crime_step$results


set.seed(110)


# fit model - lasso
crime_lasso <- train(
        Crime ~ ., 
        data = crime_train,
        method = 'glmnet',
        trControl = trainControl(method = 'cv'),
        tuneGrid=expand.grid(
              .alpha=1,
              .lambda=seq(0, 100, by = .1))
        )

# lasso coefficients with best tuned regularization parameter lambda (alpha hard coded at 1 for lasso)
coef(crime_lasso$finalModel, crime_lasso$bestTune$lambda==crime_lasso$bestTune$lambda)

# best tune performance
crime_lasso$results %>% as_tibble() %>% 
  filter(
    lambda == crime_lasso$bestTune$lambda
    )


set.seed(110)


# fit model - elastic net
crime_elastic <- train(
        Crime ~ ., 
        data = crime_train,
        method = 'glmnet',
        trControl = trainControl(method = 'cv'),
        tuneGrid=expand.grid(
              .alpha= .5,
              .lambda=seq(0, 100, by = .1))
        )

# elastic net coefficients with best tuned regularization parameter lambda and alpha
coef(
  crime_elastic$finalModel,
  crime_elastic$bestTune$lambda==crime_elastic$bestTune$lambda & 
  crime_elastic$bestTune$alpha == crime_elastic$bestTune$alpha
     )

# best tune performance
crime_elastic$results %>% as_tibble() %>% 
  filter(
    alpha == crime_elastic$bestTune$alpha, 
    lambda == crime_elastic$bestTune$lambda
    )





```

<br>

## Question 12.1

Question:

Describe a situation or problem from your job, everyday life, current events, etc., for which a design of experiments approach would be appropriate. 

<br>

Answer:

One situation I can imagine is combining a model's output with a carefully designed study to learn more about our target population. We can develop a model that scores customers with probabilities of purchase. Then we can market to these customers with the highest probability. This seems great, but what if those customers were most likely to purchase anyway, and we wasted marketing on 'sure things'? Experiment design can help us with this problem. We can group our customers into probability bands and see which probability group is more responsive to marketing emails. It may turn out that we should market to customers in the 40% range - they are on the edge of purchase and need marketing to persuade them. This combination of model output and a cleverly designed experiment and be really powerful. 


<br>




## Question 12.2

Question:

To determine the value of 10 different yes/no features to the market value of a house (large yard, solar roof, etc.), a real estate agent plans to survey 50 potential buyers, showing a fictitious house with different combinations of features.  To reduce the survey size, the agent wants to show just 16 fictitious houses. Use R's FrF2 function (in the FrF2 package) to find a fractional factorial design for this experiment: what set of features should each of the 16 fictitious houses have?  Note: the output of FrF2 is "1" (include) or  "-1" (don't include) for each feature

<br>

Answer:

Below is the implementation of the fractional factorial design for 16 open houses based on 10 features. Output below shows which house should have which features to show to prospective house buyers. 


<br>


```{r partial factor, fig.align= 'center', message = FALSE, warning=FALSE}

set.seed(40)

(houses <- FrF2(
  16, 10, 
  factor.names = c(
    'Large Yard', 'Pool', 'Solar Roof', 'Long Driveway', 'Multi Car Garage',
    'Walk-In Closet', 'Man Cave', 'Full Bar', 'Gazebo', 'Elevator'
    ),
  default.levels = c('Yes', 'No')
  ) %>% 
  as_tibble() %>% 
  rownames_to_column('House')
 )


```

## Question 13.1

Question:

For each of the following distributions, give an example of data that you would expect to follow this distribution (besides the examples already discussed in class). 
a. Binomial    b. Geometric    c. Poisson    d. Exponential   e. Weibull

<br>

Answer:

Examples of the distributions above include:


    - Binomial: probability of acceptance to a prestigious college (success p = .15)
    - Geometric: probability of passing an exam on a certain attempt (i.e we keep taking it the exam until we pass)
    - Poisson: number of visits to a certain webpage for given time period
    - Exponential: times between when cars arrive to the work parking garage
    - Weibull: how long will it take for my car battery to fail



<br>


